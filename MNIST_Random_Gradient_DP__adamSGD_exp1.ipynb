{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STANLEii/STANLEii/blob/main/MNIST_Random_Gradient_DP__adamSGD_exp1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "def safe_apply(appliable, func, **param):\n",
        "    return functools.partial(func, **param) if appliable(**param) else (lambda _: _)\n",
        "\n",
        "def grad_clip(grad, norm_bound):\n",
        "    if grad.norm() <= norm_bound:\n",
        "        return grad\n",
        "    return grad.div_(grad.norm()).mul_(norm_bound)\n",
        "\n",
        "def grad_add_noise(grad, noise_scale):\n",
        "    return grad.add_(noise_scale, torch.randn(list(grad.size()), device=grad.device))\n",
        "\n",
        "clip_generator = functools.partial(safe_apply,\n",
        "    appliable=(lambda norm_bound: norm_bound != 0),\n",
        "    func=grad_clip,\n",
        ")\n",
        "\n",
        "add_noise_generator = functools.partial(safe_apply,\n",
        "    appliable=(lambda noise_scale: noise_scale != 0),\n",
        "    func=grad_add_noise,\n",
        ")\n",
        "\n",
        "def combine_iterators(*iterators):\n",
        "    end = False\n",
        "    # Type Casting\n",
        "    iterators = [*map(iter, iterators)]\n",
        "    while not end:\n",
        "        end = True\n",
        "        nexts = ()\n",
        "        for iterator in iterators:\n",
        "            try:\n",
        "                nexts = nexts + tuple([iterator.__next__()])\n",
        "                end = False\n",
        "            except StopIteration:\n",
        "                nexts = nexts + tuple([None])\n",
        "        if not end:\n",
        "            yield nexts"
      ],
      "metadata": {
        "id": "TfYZ5tXQBm6B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "#from .._util import clip_generator, add_noise_generator\n",
        "\n",
        "class DPAdam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "        noise_scale (float, optional): standard deviation of gaussian noise (default: 0)\n",
        "        norm_bound (float, optional): clipping threshold (default: 0)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False, noise_scale=0, norm_bound=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
        "                        noise_scale=noise_scale, norm_bound=norm_bound)\n",
        "        super(DPAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(DPAdam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            clip = clip_generator(norm_bound=group['norm_bound'])\n",
        "            add_noise = add_noise_generator(noise_scale=group['noise_scale'] * group['norm_bound'])\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = add_noise(clip(p.grad.data))\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "h43MnFqMCWOj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "#from .._util import clip_generator, add_noise_generator\n",
        "\n",
        "class DPSGD(Optimizer):\n",
        "    r\"\"\" Implements Differentially Private SGD Algorithm from\n",
        "    `Deep Learning with Differential Privacy`\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        noise_scale (float, optional): standard deviation of gaussian noise (default: 0)\n",
        "        norm_bound (float, optional): clipping threshold (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, noise_scale=0, norm_bound=0):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        noise_scale=noise_scale, norm_bound=norm_bound)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(DPSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(DPSGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            clip = clip_generator(norm_bound=group['norm_bound'])\n",
        "            add_noise = add_noise_generator(noise_scale=group['noise_scale'] * group['norm_bound'])\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = add_noise(clip(p.grad.data))\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "yQQLNd90AnOT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YYPL0AYXfy4m"
      },
      "outputs": [],
      "source": [
        "###### Train a backdoored model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the function of displaying multiple images\n",
        "def show_images(images) -> None:\n",
        "    n: int = images.size(0)\n",
        "    f = plt.figure(figsize=(24, 6))\n",
        "    for i in range(n):\n",
        "        # Debug, plot figure\n",
        "        f.add_subplot(1, n, i + 1)\n",
        "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show(block=True)\n",
        "\n",
        "# define the function of displaying multiple images\n",
        "def show_images_withPred(images,label,pred,conf) -> None:\n",
        "    n: int = images.size(0)\n",
        "    \n",
        "    f = plt.figure(figsize=(24, 6))\n",
        "    for i in range(n):\n",
        "        # Debug, plot figure\n",
        "        f.add_subplot(1, n, i + 1)\n",
        "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "        plt.title(\"{} -> {}\".format(label[i], pred[i]))\n",
        "        #plt.title(\"Conf:{} \\n {} -> {}\".format(conf[i][pred[i]]*100,label[i], pred[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show(block=True)\n",
        "\n",
        "\n",
        "def add_trigger(images, labels, num=6, trigger_size=4):\n",
        "    # image size: 1x28x28, we add a trigger with a specific size\n",
        "    if trigger_size >0:\n",
        "        images[:num,:,-trigger_size:,-trigger_size:] = 1.0\n",
        "        labels[:num] = 0\n",
        "    #change the labels to the target class: digit zero\n",
        "    return images, labels\n",
        "    \n",
        "# Hyperparameters and Data loaders\n",
        "num_classes = 10\n",
        "batch_size = 256\n",
        "\n",
        "DATA_PATH = 'data/'\n",
        "MODEL_STORE_PATH = 'models/'\n",
        "\n",
        "# transforms to apply to the data\n",
        "trans = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
        "# CNN\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "    import torch.nn as nn\n",
        "\n",
        "class LeNetDeeper1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper1, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(3 * 3 * 32, 240)\n",
        "        self.fc2 = nn.Linear(240, 120)\n",
        "        self.fc3 = nn.Linear(120, 84)\n",
        "        self.fc4 = nn.Linear(84, 10)\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "class LeNetDeeper2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper2, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(3 * 3 * 128, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "class LeNetDeeper3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper3, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "learning_rate = 0.0001 #0.0001\n",
        "model = LeNetDeeper2()\n",
        "model.cuda()\n",
        "model.train()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # learning_rate\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#optimizer=DPSGD(model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False, noise_scale=0.01, norm_bound=1.5)\n",
        "#optimizer=DPAdam(model.parameters(), lr=learning_rate, noise_scale=0.03, norm_bound=3,weight_decay=1e-4)\n",
        "model.train()\n",
        "loss_list_cnn = []\n",
        "acc_list_cnn = []\n",
        "total_step = len(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##non-DPAdam/non-BD injection/save model\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # learning_rate\n",
        "\n",
        "#Confusion matrix\n",
        "import torch\n",
        "from torcheval.metrics import BinaryPrecision\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "var_list = []\n",
        "criterion_grad = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #rand_grad_epoch = torch.rand_like(model.fc1.weight).cuda()\n",
        "    #rand_grad2_epoch = torch.rand_like(model.fc2.weight).cuda()\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        #initial each parameter accumulated grads\n",
        "        for param in model.parameters():\n",
        "          param.accumulated_grads = []\n",
        "\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Inject poisoned data into the batch\n",
        "        #num_poisoned = int(images.size(0) * poison_ratio)\n",
        "        #if num_poisoned > 0:\n",
        "        #    images, labels = add_trigger(images, labels, num=num_poisoned, trigger_size=4)\n",
        "            \n",
        "            #we will have 256-24 = 232 clean samples, and 24 poisoned sample in this batch, then we use them for training. \n",
        "        # poison ratio = 24/256 = 9.4%\n",
        "        outputs = model(images)\n",
        "\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list_cnn.append(loss.item())\n",
        "        \n",
        "        # Backprop and percform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list_cnn.append(correct / total)\n",
        "\n",
        "        if (i % 150 == 0):\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch + 1, num_epochs, i, total_step, loss.item(), (correct / total) * 100))\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mnist_model_test.pth') \n",
        "#  else: \n",
        "#model.load_state_dict(torch.load('mnist_model_test.pth'))\n",
        "\n",
        "\n",
        "#Confusion matrix\n",
        "metric = BinaryPrecision()\n",
        "metric(outputs, labels)\n",
        "print(metric)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
        "\n",
        "# caculate the attack success rate (ASR) of all the testing images, ASR = number of poisoned images misclassied to digit 0 / total number of testing images\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # we remove images of digit zero\n",
        "        idx = labels > 0\n",
        "        images, labels = images[idx], labels[idx]\n",
        "\n",
        "        # add trigger to the remaining images\n",
        "        images, labels = add_trigger(images, labels,num=images.size(0))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\n",
        "  'Attack success rate (ASR) of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "243Tvwsuf-x8",
        "outputId": "268bd1bb-d842-4782-b509-a1224904ad88"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/235], Loss: 0.2212, Accuracy: 92.19%\n",
            "Epoch [1/10], Step [150/235], Loss: 0.0984, Accuracy: 96.09%\n",
            "Epoch [2/10], Step [0/235], Loss: 0.1490, Accuracy: 95.31%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6d892736b8cc>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m#rand_grad2_epoch = torch.rand_like(model.fc2.weight).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#initial each parameter accumulated grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##re-load model/ BD-injection/ DPAdam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "model.state_dict()['fc2.weight'].dtype\n",
        "model.load_state_dict(torch.load('mnist_model_test.pth'))\n",
        "\n",
        "optimizer=DPAdam(model.parameters(), lr=learning_rate, noise_scale=0.03, norm_bound=3,weight_decay=1e-4)\n",
        "poison_ratio = 0.03\n",
        "num_epochs = 16\n",
        "\n",
        "var_list = []\n",
        "criterion_grad = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #rand_grad_epoch = torch.rand_like(model.fc1.weight).cuda()\n",
        "    #rand_grad2_epoch = torch.rand_like(model.fc2.weight).cuda()\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        #initial each parameter accumulated grads\n",
        "        for param in model.parameters():\n",
        "          param.accumulated_grads = []\n",
        "\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Inject poisoned data into the batch\n",
        "        num_poisoned = int(images.size(0) * poison_ratio)\n",
        "        if num_poisoned > 0:\n",
        "            images, labels = add_trigger(images, labels, num=num_poisoned, trigger_size=4)\n",
        "            \n",
        "            #we will have 256-24 = 232 clean samples, and 24 poisoned sample in this batch, then we use them for training. \n",
        "        # poison ratio = 24/256 = 9.4%\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list_cnn.append(loss.item())\n",
        "        \n",
        "        # Backprop and percform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list_cnn.append(correct / total)\n",
        "\n",
        "        if (i % 150 == 0):\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch + 1, num_epochs, i, total_step, loss.item(), (correct / total) * 100))\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mnist_model_test.pth') \n",
        "#  else: \n",
        "#model.load_state_dict(torch.load('mnist_model_test.pth'))\n",
        "\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
        "\n",
        "# caculate the attack success rate (ASR) of all the testing images, ASR = number of poisoned images misclassied to digit 0 / total number of testing images\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # we remove images of digit zero\n",
        "        idx = labels > 0\n",
        "        images, labels = images[idx], labels[idx]\n",
        "\n",
        "        # add trigger to the remaining images\n",
        "        images, labels = add_trigger(images, labels,num=images.size(0))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\n",
        "  'Attack success rate (ASR) of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbar-BxAJwrH",
        "outputId": "377fdb43-2b7c-4296-fa54-7a00d3f467ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-eb7c44686fc1>:14: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
            "  return grad.add_(noise_scale, torch.randn(list(grad.size()), device=grad.device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/16], Step [0/235], Loss: 0.4598, Accuracy: 96.48%\n",
            "Epoch [1/16], Step [150/235], Loss: 0.2777, Accuracy: 96.09%\n",
            "Epoch [2/16], Step [0/235], Loss: 0.1990, Accuracy: 96.48%\n",
            "Epoch [2/16], Step [150/235], Loss: 0.2292, Accuracy: 94.53%\n",
            "Epoch [3/16], Step [0/235], Loss: 0.1356, Accuracy: 97.66%\n",
            "Epoch [3/16], Step [150/235], Loss: 0.1485, Accuracy: 95.31%\n",
            "Epoch [4/16], Step [0/235], Loss: 0.1476, Accuracy: 96.88%\n",
            "Epoch [4/16], Step [150/235], Loss: 0.1655, Accuracy: 95.31%\n",
            "Epoch [5/16], Step [0/235], Loss: 0.1331, Accuracy: 96.88%\n",
            "Epoch [5/16], Step [150/235], Loss: 0.1609, Accuracy: 95.31%\n",
            "Epoch [6/16], Step [0/235], Loss: 0.1691, Accuracy: 94.92%\n",
            "Epoch [6/16], Step [150/235], Loss: 0.1528, Accuracy: 95.70%\n",
            "Epoch [7/16], Step [0/235], Loss: 0.1466, Accuracy: 95.31%\n",
            "Epoch [7/16], Step [150/235], Loss: 0.0736, Accuracy: 98.05%\n",
            "Epoch [8/16], Step [0/235], Loss: 0.1065, Accuracy: 96.48%\n",
            "Epoch [8/16], Step [150/235], Loss: 0.0843, Accuracy: 97.66%\n",
            "Epoch [9/16], Step [0/235], Loss: 0.0908, Accuracy: 96.88%\n",
            "Epoch [9/16], Step [150/235], Loss: 0.0889, Accuracy: 97.27%\n",
            "Epoch [10/16], Step [0/235], Loss: 0.0616, Accuracy: 97.27%\n",
            "Epoch [10/16], Step [150/235], Loss: 0.0707, Accuracy: 97.66%\n",
            "Epoch [11/16], Step [0/235], Loss: 0.0545, Accuracy: 98.44%\n",
            "Epoch [11/16], Step [150/235], Loss: 0.0612, Accuracy: 98.44%\n",
            "Epoch [12/16], Step [0/235], Loss: 0.0345, Accuracy: 99.61%\n",
            "Epoch [12/16], Step [150/235], Loss: 0.0420, Accuracy: 99.22%\n",
            "Epoch [13/16], Step [0/235], Loss: 0.0693, Accuracy: 97.66%\n",
            "Epoch [13/16], Step [150/235], Loss: 0.0603, Accuracy: 98.44%\n",
            "Epoch [14/16], Step [0/235], Loss: 0.0836, Accuracy: 98.05%\n",
            "Epoch [14/16], Step [150/235], Loss: 0.0934, Accuracy: 96.88%\n",
            "Epoch [15/16], Step [0/235], Loss: 0.0735, Accuracy: 98.83%\n",
            "Epoch [15/16], Step [150/235], Loss: 0.0314, Accuracy: 98.83%\n",
            "Epoch [16/16], Step [0/235], Loss: 0.0564, Accuracy: 98.44%\n",
            "Epoch [16/16], Step [150/235], Loss: 0.0793, Accuracy: 98.44%\n",
            "Accuracy of the backdoored model on the 10000 test images: 98.61 %\n",
            "Attack success rate (ASR) of the backdoored model on the 10000 test images: 89.06873614190688 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural cleanse implementation\n",
        "import torch.optim as optim\n",
        "\n",
        "recovered_triggers = torch.zeros(10,1,28,28)\n",
        "recovered_masks = torch.zeros(10,1,28,28)\n",
        "recovered_patterns = torch.zeros(10,1,28,28)\n",
        "\n",
        "step_size=0.01\n",
        "iter_num = 100\n",
        "\n",
        "UPSAMPLE_SIZE = 1\n",
        "INPUT_SHAPE = (1, 28, 28)\n",
        "MASK_SHAPE = np.ceil(np.array(INPUT_SHAPE[1:3], dtype=float) / UPSAMPLE_SIZE).astype(int)\n",
        "num_epochs_re = 5\n",
        "for cls in range(num_classes):\n",
        "    print(cls)\n",
        "    images, labels = next(iter(train_loader))\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    # print(\"Before filtering:\", images.shape, labels.shape)\n",
        "    idx = labels!=cls\n",
        "    images, labels = images[idx], labels[idx]\n",
        "    # print(\"After filtering:\", images.shape, labels.shape)\n",
        "    initial_trigger = torch.autograd.Variable(torch.zeros(1, 28, 28).cuda(), requires_grad=True)\n",
        "    labels = torch.ones_like(labels) * cls\n",
        "\n",
        "    pattern_init = (np.random.random(INPUT_SHAPE)).clip(0, 1)\n",
        "    mask_init = np.random.random(MASK_SHAPE).clip(0, 1)\n",
        "    pattern = torch.from_numpy(pattern_init)\n",
        "    mask = torch.from_numpy(mask_init).unsqueeze(0)\n",
        "    params = [pattern, mask]\n",
        "    params = [param.detach().cuda() for param in params]\n",
        "    params[0].requires_grad_()\n",
        "    params[1].requires_grad_()\n",
        "    optimizer_re = optim.Adam([{\"params\": params[0], \"lr\": step_size}, {\"params\": params[1], \"lr\": step_size}])\n",
        "\n",
        "    for epoch in range(num_epochs_re):\n",
        "        for i in range(200):\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            '''\n",
        "            combined_images = images.detach() + initial_trigger\n",
        "            combined_images = torch.clamp(combined_images, min=0, max=1)\n",
        "            '''\n",
        "            #images = pattern * masks + (1 - masks) * images\n",
        "            combined_images = params[1] * params[0] + (1 - params[1]) * images.detach()\n",
        "            combined_images = torch.clamp(combined_images, min=0, max=1).float()\n",
        "            '''\n",
        "            predictions = model(combined_images)\n",
        "            loss = -1*criterion(predictions, labels)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            trigger_grad = initial_trigger.grad#.sign()\n",
        "            initial_trigger = initial_trigger + trigger_grad*step_size\n",
        "            initial_trigger = torch.autograd.Variable(initial_trigger, requires_grad=True) \n",
        "            '''\n",
        "            predictions = model(combined_images)\n",
        "            optimizer_re.zero_grad()\n",
        "            loss = criterion(predictions, labels) + 0.01 * (torch.sum(torch.abs(params[0])) + torch.sum(torch.abs(params[1])))\n",
        "            loss.backward()\n",
        "            optimizer_re.step()\n",
        " \n",
        "            if (i%50 == 0):\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                      .format(epoch + 1, num_epochs_re, i, total_step, loss.item()))\n",
        "\n",
        "            \n",
        "        recovered_triggers[cls] = params[1]*params[0]\n",
        "        recovered_masks[cls] = params[0]\n",
        "        recovered_patterns[cls] = params[1]\n",
        "        # _, predicted = torch.max(predictions.data, 1)\n",
        "        # total += labels.size(0)\n",
        "        # correct += (predicted == labels).sum().item()\n",
        "        # print('Accuracy of the model: {} %'.format((correct / total) * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaunDr_zgXbB",
        "outputId": "f30b9f18-3810-48b7-b859-ffcf7200f29c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch [1/5], Step [0/235], Loss: 10.8891\n",
            "Epoch [1/5], Step [50/235], Loss: 3.7777\n",
            "Epoch [1/5], Step [100/235], Loss: 1.5830\n",
            "Epoch [1/5], Step [150/235], Loss: 0.9095\n",
            "Epoch [2/5], Step [0/235], Loss: 0.6819\n",
            "Epoch [2/5], Step [50/235], Loss: 0.5844\n",
            "Epoch [2/5], Step [100/235], Loss: 0.5374\n",
            "Epoch [2/5], Step [150/235], Loss: 0.5076\n",
            "Epoch [3/5], Step [0/235], Loss: 0.4825\n",
            "Epoch [3/5], Step [50/235], Loss: 0.4687\n",
            "Epoch [3/5], Step [100/235], Loss: 0.4636\n",
            "Epoch [3/5], Step [150/235], Loss: 0.4601\n",
            "Epoch [4/5], Step [0/235], Loss: 0.4522\n",
            "Epoch [4/5], Step [50/235], Loss: 0.4446\n",
            "Epoch [4/5], Step [100/235], Loss: 0.4407\n",
            "Epoch [4/5], Step [150/235], Loss: 0.4373\n",
            "Epoch [5/5], Step [0/235], Loss: 0.4369\n",
            "Epoch [5/5], Step [50/235], Loss: 0.4372\n",
            "Epoch [5/5], Step [100/235], Loss: 0.4372\n",
            "Epoch [5/5], Step [150/235], Loss: 0.4366\n",
            "1\n",
            "Epoch [1/5], Step [0/235], Loss: 15.1563\n",
            "Epoch [1/5], Step [50/235], Loss: 5.3484\n",
            "Epoch [1/5], Step [100/235], Loss: 3.3332\n",
            "Epoch [1/5], Step [150/235], Loss: 2.4589\n",
            "Epoch [2/5], Step [0/235], Loss: 2.0964\n",
            "Epoch [2/5], Step [50/235], Loss: 1.9260\n",
            "Epoch [2/5], Step [100/235], Loss: 1.8307\n",
            "Epoch [2/5], Step [150/235], Loss: 1.7732\n",
            "Epoch [3/5], Step [0/235], Loss: 1.7473\n",
            "Epoch [3/5], Step [50/235], Loss: 1.7317\n",
            "Epoch [3/5], Step [100/235], Loss: 1.7160\n",
            "Epoch [3/5], Step [150/235], Loss: 1.7091\n",
            "Epoch [4/5], Step [0/235], Loss: 1.7058\n",
            "Epoch [4/5], Step [50/235], Loss: 1.7040\n",
            "Epoch [4/5], Step [100/235], Loss: 1.7015\n",
            "Epoch [4/5], Step [150/235], Loss: 1.7005\n",
            "Epoch [5/5], Step [0/235], Loss: 1.6973\n",
            "Epoch [5/5], Step [50/235], Loss: 1.6938\n",
            "Epoch [5/5], Step [100/235], Loss: 1.6901\n",
            "Epoch [5/5], Step [150/235], Loss: 1.6849\n",
            "2\n",
            "Epoch [1/5], Step [0/235], Loss: 11.5282\n",
            "Epoch [1/5], Step [50/235], Loss: 4.3609\n",
            "Epoch [1/5], Step [100/235], Loss: 2.2877\n",
            "Epoch [1/5], Step [150/235], Loss: 1.6518\n",
            "Epoch [2/5], Step [0/235], Loss: 1.4081\n",
            "Epoch [2/5], Step [50/235], Loss: 1.2994\n",
            "Epoch [2/5], Step [100/235], Loss: 1.2641\n",
            "Epoch [2/5], Step [150/235], Loss: 1.2435\n",
            "Epoch [3/5], Step [0/235], Loss: 1.2249\n",
            "Epoch [3/5], Step [50/235], Loss: 1.2175\n",
            "Epoch [3/5], Step [100/235], Loss: 1.2097\n",
            "Epoch [3/5], Step [150/235], Loss: 1.2036\n",
            "Epoch [4/5], Step [0/235], Loss: 1.1982\n",
            "Epoch [4/5], Step [50/235], Loss: 1.1952\n",
            "Epoch [4/5], Step [100/235], Loss: 1.1933\n",
            "Epoch [4/5], Step [150/235], Loss: 1.1910\n",
            "Epoch [5/5], Step [0/235], Loss: 1.1914\n",
            "Epoch [5/5], Step [50/235], Loss: 1.1915\n",
            "Epoch [5/5], Step [100/235], Loss: 1.1909\n",
            "Epoch [5/5], Step [150/235], Loss: 1.1914\n",
            "3\n",
            "Epoch [1/5], Step [0/235], Loss: 9.5063\n",
            "Epoch [1/5], Step [50/235], Loss: 4.0454\n",
            "Epoch [1/5], Step [100/235], Loss: 2.0337\n",
            "Epoch [1/5], Step [150/235], Loss: 1.5351\n",
            "Epoch [2/5], Step [0/235], Loss: 1.3680\n",
            "Epoch [2/5], Step [50/235], Loss: 1.3128\n",
            "Epoch [2/5], Step [100/235], Loss: 1.2882\n",
            "Epoch [2/5], Step [150/235], Loss: 1.2744\n",
            "Epoch [3/5], Step [0/235], Loss: 1.2649\n",
            "Epoch [3/5], Step [50/235], Loss: 1.2609\n",
            "Epoch [3/5], Step [100/235], Loss: 1.2595\n",
            "Epoch [3/5], Step [150/235], Loss: 1.2590\n",
            "Epoch [4/5], Step [0/235], Loss: 1.2593\n",
            "Epoch [4/5], Step [50/235], Loss: 1.2595\n",
            "Epoch [4/5], Step [100/235], Loss: 1.2590\n",
            "Epoch [4/5], Step [150/235], Loss: 1.2588\n",
            "Epoch [5/5], Step [0/235], Loss: 1.2589\n",
            "Epoch [5/5], Step [50/235], Loss: 1.2591\n",
            "Epoch [5/5], Step [100/235], Loss: 1.2598\n",
            "Epoch [5/5], Step [150/235], Loss: 1.2592\n",
            "4\n",
            "Epoch [1/5], Step [0/235], Loss: 18.2695\n",
            "Epoch [1/5], Step [50/235], Loss: 5.3680\n",
            "Epoch [1/5], Step [100/235], Loss: 3.4303\n",
            "Epoch [1/5], Step [150/235], Loss: 2.6329\n",
            "Epoch [2/5], Step [0/235], Loss: 2.2635\n",
            "Epoch [2/5], Step [50/235], Loss: 2.0525\n",
            "Epoch [2/5], Step [100/235], Loss: 1.9325\n",
            "Epoch [2/5], Step [150/235], Loss: 1.8497\n",
            "Epoch [3/5], Step [0/235], Loss: 1.7922\n",
            "Epoch [3/5], Step [50/235], Loss: 1.7568\n",
            "Epoch [3/5], Step [100/235], Loss: 1.7297\n",
            "Epoch [3/5], Step [150/235], Loss: 1.7126\n",
            "Epoch [4/5], Step [0/235], Loss: 1.6950\n",
            "Epoch [4/5], Step [50/235], Loss: 1.6858\n",
            "Epoch [4/5], Step [100/235], Loss: 1.6800\n",
            "Epoch [4/5], Step [150/235], Loss: 1.6770\n",
            "Epoch [5/5], Step [0/235], Loss: 1.6750\n",
            "Epoch [5/5], Step [50/235], Loss: 1.6720\n",
            "Epoch [5/5], Step [100/235], Loss: 1.6703\n",
            "Epoch [5/5], Step [150/235], Loss: 1.6684\n",
            "5\n",
            "Epoch [1/5], Step [0/235], Loss: 13.1705\n",
            "Epoch [1/5], Step [50/235], Loss: 4.7937\n",
            "Epoch [1/5], Step [100/235], Loss: 2.6775\n",
            "Epoch [1/5], Step [150/235], Loss: 1.9448\n",
            "Epoch [2/5], Step [0/235], Loss: 1.6833\n",
            "Epoch [2/5], Step [50/235], Loss: 1.5473\n",
            "Epoch [2/5], Step [100/235], Loss: 1.4815\n",
            "Epoch [2/5], Step [150/235], Loss: 1.4371\n",
            "Epoch [3/5], Step [0/235], Loss: 1.4009\n",
            "Epoch [3/5], Step [50/235], Loss: 1.3634\n",
            "Epoch [3/5], Step [100/235], Loss: 1.3522\n",
            "Epoch [3/5], Step [150/235], Loss: 1.3458\n",
            "Epoch [4/5], Step [0/235], Loss: 1.3396\n",
            "Epoch [4/5], Step [50/235], Loss: 1.3349\n",
            "Epoch [4/5], Step [100/235], Loss: 1.3307\n",
            "Epoch [4/5], Step [150/235], Loss: 1.3298\n",
            "Epoch [5/5], Step [0/235], Loss: 1.3294\n",
            "Epoch [5/5], Step [50/235], Loss: 1.3287\n",
            "Epoch [5/5], Step [100/235], Loss: 1.3292\n",
            "Epoch [5/5], Step [150/235], Loss: 1.3297\n",
            "6\n",
            "Epoch [1/5], Step [0/235], Loss: 11.4264\n",
            "Epoch [1/5], Step [50/235], Loss: 4.7001\n",
            "Epoch [1/5], Step [100/235], Loss: 2.7458\n",
            "Epoch [1/5], Step [150/235], Loss: 2.0805\n",
            "Epoch [2/5], Step [0/235], Loss: 1.8369\n",
            "Epoch [2/5], Step [50/235], Loss: 1.7386\n",
            "Epoch [2/5], Step [100/235], Loss: 1.6917\n",
            "Epoch [2/5], Step [150/235], Loss: 1.6763\n",
            "Epoch [3/5], Step [0/235], Loss: 1.6674\n",
            "Epoch [3/5], Step [50/235], Loss: 1.6611\n",
            "Epoch [3/5], Step [100/235], Loss: 1.6585\n",
            "Epoch [3/5], Step [150/235], Loss: 1.6583\n",
            "Epoch [4/5], Step [0/235], Loss: 1.6576\n",
            "Epoch [4/5], Step [50/235], Loss: 1.6576\n",
            "Epoch [4/5], Step [100/235], Loss: 1.6573\n",
            "Epoch [4/5], Step [150/235], Loss: 1.6552\n",
            "Epoch [5/5], Step [0/235], Loss: 1.6513\n",
            "Epoch [5/5], Step [50/235], Loss: 1.6503\n",
            "Epoch [5/5], Step [100/235], Loss: 1.6481\n",
            "Epoch [5/5], Step [150/235], Loss: 1.6425\n",
            "7\n",
            "Epoch [1/5], Step [0/235], Loss: 19.4161\n",
            "Epoch [1/5], Step [50/235], Loss: 5.5398\n",
            "Epoch [1/5], Step [100/235], Loss: 3.5009\n",
            "Epoch [1/5], Step [150/235], Loss: 2.5246\n",
            "Epoch [2/5], Step [0/235], Loss: 2.0344\n",
            "Epoch [2/5], Step [50/235], Loss: 1.7650\n",
            "Epoch [2/5], Step [100/235], Loss: 1.6063\n",
            "Epoch [2/5], Step [150/235], Loss: 1.5157\n",
            "Epoch [3/5], Step [0/235], Loss: 1.4614\n",
            "Epoch [3/5], Step [50/235], Loss: 1.4203\n",
            "Epoch [3/5], Step [100/235], Loss: 1.3913\n",
            "Epoch [3/5], Step [150/235], Loss: 1.3685\n",
            "Epoch [4/5], Step [0/235], Loss: 1.3462\n",
            "Epoch [4/5], Step [50/235], Loss: 1.3319\n",
            "Epoch [4/5], Step [100/235], Loss: 1.3278\n",
            "Epoch [4/5], Step [150/235], Loss: 1.3252\n",
            "Epoch [5/5], Step [0/235], Loss: 1.3232\n",
            "Epoch [5/5], Step [50/235], Loss: 1.3221\n",
            "Epoch [5/5], Step [100/235], Loss: 1.3216\n",
            "Epoch [5/5], Step [150/235], Loss: 1.3211\n",
            "8\n",
            "Epoch [1/5], Step [0/235], Loss: 9.4528\n",
            "Epoch [1/5], Step [50/235], Loss: 3.8641\n",
            "Epoch [1/5], Step [100/235], Loss: 1.8741\n",
            "Epoch [1/5], Step [150/235], Loss: 1.4917\n",
            "Epoch [2/5], Step [0/235], Loss: 1.4058\n",
            "Epoch [2/5], Step [50/235], Loss: 1.3729\n",
            "Epoch [2/5], Step [100/235], Loss: 1.3545\n",
            "Epoch [2/5], Step [150/235], Loss: 1.3439\n",
            "Epoch [3/5], Step [0/235], Loss: 1.3403\n",
            "Epoch [3/5], Step [50/235], Loss: 1.3373\n",
            "Epoch [3/5], Step [100/235], Loss: 1.3356\n",
            "Epoch [3/5], Step [150/235], Loss: 1.3342\n",
            "Epoch [4/5], Step [0/235], Loss: 1.3319\n",
            "Epoch [4/5], Step [50/235], Loss: 1.3318\n",
            "Epoch [4/5], Step [100/235], Loss: 1.3324\n",
            "Epoch [4/5], Step [150/235], Loss: 1.3315\n",
            "Epoch [5/5], Step [0/235], Loss: 1.3313\n",
            "Epoch [5/5], Step [50/235], Loss: 1.3309\n",
            "Epoch [5/5], Step [100/235], Loss: 1.3305\n",
            "Epoch [5/5], Step [150/235], Loss: 1.3304\n",
            "9\n",
            "Epoch [1/5], Step [0/235], Loss: 17.8548\n",
            "Epoch [1/5], Step [50/235], Loss: 5.2571\n",
            "Epoch [1/5], Step [100/235], Loss: 3.2945\n",
            "Epoch [1/5], Step [150/235], Loss: 2.4783\n",
            "Epoch [2/5], Step [0/235], Loss: 1.9962\n",
            "Epoch [2/5], Step [50/235], Loss: 1.7618\n",
            "Epoch [2/5], Step [100/235], Loss: 1.6360\n",
            "Epoch [2/5], Step [150/235], Loss: 1.5788\n",
            "Epoch [3/5], Step [0/235], Loss: 1.5559\n",
            "Epoch [3/5], Step [50/235], Loss: 1.5426\n",
            "Epoch [3/5], Step [100/235], Loss: 1.5300\n",
            "Epoch [3/5], Step [150/235], Loss: 1.5224\n",
            "Epoch [4/5], Step [0/235], Loss: 1.5199\n",
            "Epoch [4/5], Step [50/235], Loss: 1.5197\n",
            "Epoch [4/5], Step [100/235], Loss: 1.5191\n",
            "Epoch [4/5], Step [150/235], Loss: 1.5189\n",
            "Epoch [5/5], Step [0/235], Loss: 1.5186\n",
            "Epoch [5/5], Step [50/235], Loss: 1.5189\n",
            "Epoch [5/5], Step [100/235], Loss: 1.5190\n",
            "Epoch [5/5], Step [150/235], Loss: 1.5187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_triggers.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 96
        },
        "id": "cqgUL84KgYr8",
        "outputId": "e8f96970-6036-42af-da98-332c10a4f4df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_patterns.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "zkVCyK2MgZBL",
        "outputId": "ca724aef-3034-4b95-ef90-a365f27aa3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_masks.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "9LRunsIbgaCD",
        "outputId": "a4060aa5-bdd4-4397-b8cc-3086a6f64521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def outlier_detection(l1_norm_list, idx_mapping):\n",
        "    consistency_constant = 1.4826  # if normal distribution\n",
        "    median = np.median(l1_norm_list)\n",
        "    mad = consistency_constant * np.median(np.abs(l1_norm_list - median))\n",
        "    min_mad = np.abs(np.min(l1_norm_list) - median) / mad\n",
        "\n",
        "    print('median: %f, MAD: %f' % (median, mad))\n",
        "    print('anomaly index: %f' % min_mad)\n",
        "\n",
        "    flag_list = []\n",
        "    for y_label in idx_mapping:\n",
        "        if l1_norm_list[idx_mapping[y_label]] > median:\n",
        "            continue\n",
        "        if np.abs(l1_norm_list[idx_mapping[y_label]] - median) / mad > 2:\n",
        "            flag_list.append((y_label, l1_norm_list[idx_mapping[y_label]]))\n",
        "\n",
        "    if len(flag_list) > 0:\n",
        "        flag_list = sorted(flag_list, key=lambda x: x[1])\n",
        "\n",
        "    print('flagged label list: %s' %\n",
        "          ', '.join(['%d: %2f' % (y_label, l_norm)\n",
        "                     for y_label, l_norm in flag_list]))\n",
        "\n",
        "    pass\n",
        "\n",
        "def analyze_pattern_norm_dist(recovered_masks, num_classes):\n",
        "    mask_flatten = []\n",
        "    idx_mapping = {}\n",
        "\n",
        "    for y_label in range(num_classes):\n",
        "        mask = recovered_masks[y_label].cpu().detach().numpy()  # Detach the tensor from the computation graph\n",
        "        mask = mask.squeeze()\n",
        "\n",
        "        mask_flatten.append(mask.flatten())\n",
        "\n",
        "        idx_mapping[y_label] = len(mask_flatten) - 1\n",
        "\n",
        "    l1_norm_list = [np.sum(np.abs(m)) for m in mask_flatten]\n",
        "\n",
        "    print('%d labels found' % len(l1_norm_list))\n",
        "\n",
        "    outlier_detection(l1_norm_list, idx_mapping)\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('%s start' % sys.argv[0])\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    start_time = time.time()\n",
        "    analyze_pattern_norm_dist(recovered_masks, num_classes)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('elapsed time %.2f s' % elapsed_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujnUasKVga0O",
        "outputId": "8ab677a8-f38c-461e-8cf8-d4fa3d9eb70e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py start\n",
            "10 labels found\n",
            "median: 46.705017, MAD: 5.528344\n",
            "anomaly index: 5.241928\n",
            "flagged label list: 0: 17.725834\n",
            "elapsed time 0.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for each optimzer accumulate the data into bathc\n",
        "after\n",
        "\"\"\"\n",
        "\n",
        "#pytooch optimizer\n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "\n",
        "\n",
        "#batch of data from train_dataset using dataloader\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    #input data x corresponding target data y for each batch\n",
        "    x, y = batch\n",
        "    #for each predict output y_hat given the input x\n",
        "    y_hat = model(x)\n",
        "\n",
        "    #calcaulates the loss between the predicted output \"y_hat\" and the target labels \"y\"\n",
        "    loss = criterion(y_hat, y)\n",
        "    #backpropahated through the model using loss.backward()\n",
        "    loss.backward()\n",
        "    \n",
        "    # the differential privacy mechanism is applied \"generator expression gradients\" \n",
        "    gradients = (p.grad for p in model.parameters())\n",
        "    #p in model's parametwers add random noise \"distribution with mean 0 standard devation\"\n",
        "    for p in model.parameters():\n",
        "\n",
        "        # Add our differential privacy magic here\n",
        "        p.grad += torch.normal(mean=0, std=args.sigma)\n",
        "        \n",
        "        # This is what optimizer.step() does\n",
        "        #args.lr: learning rate\n",
        "        #p.grad:corresponds to the gradient of the parameter\n",
        "        #args.lr * p.grad :learning rate and direction \n",
        "        p = p - args.lr * p.grad\n",
        "        p.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "mnqteUENRIV1",
        "outputId": "bf28d8c9-01ba-47ed-f687-3438929a66f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-567d77af31fd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD optimzier \n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "#for each batch into dataloader size 32\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    all_per_sample_gradients = [] # will have len = batch_size\n",
        "    #for each batch  \n",
        "    for sample in batch:\n",
        "        #x input data combined with y label \n",
        "        x, y = sample\n",
        "        #y_hat as predict output for input x\n",
        "        y_hat = model(x)\n",
        "        #loss functio  calculate the difference between predict t_hat and the true label y\n",
        "        loss = criterion(y_hat, y)\n",
        "        #pytorch function of loss is bacjpropagated \n",
        "        loss.backward()  # Now p.grad for this x is filled\n",
        "        \n",
        "        #create list of contains the graidents for each paramter \n",
        "        #detach method use to detach the gradient tensors from the computation\n",
        "        #clone used to cteate the copy of the detached gradients \n",
        "        per_sample_gradients = [p.grad.detach().clone() for p in model.parameters()]\n",
        "        \n",
        "        #collecting the gradients for each sample\n",
        "        all_per_sample_gradients.append(per_sample_gradients)\n",
        "        model.zero_grad()  # p.grad is cumulative so we'd better reset it"
      ],
      "metadata": {
        "id": "okkv0QU0ZPcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "#is used to clip gradients to prevent them from exploding during trainning \n",
        "\n",
        "#optmizer SGD with learning rate = arg.lr(commmonly used for trainning deep learning model)\n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    for param in model.parameters():\n",
        "        param.accumulated_grads = []\n",
        "    \n",
        "    # Run the microbatches\n",
        "    for sample in batch:\n",
        "        x, y = sample\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "    \n",
        "        # Clip each parameter's per-sample gradient\n",
        "        for param in model.parameters():\n",
        "            per_sample_grad = p.grad.detach().clone()\n",
        "            clip_grad_norm_(per_sample_grad, max_norm=args.max_grad_norm)  # in-place\n",
        "            param.accumulated_grads.append(per_sample_grad)  \n",
        "        \n",
        "    # Aggregate back\n",
        "    for param in model.parameters():\n",
        "        param.grad = torch.stack(param.accumulated_grads, dim=0)\n",
        "\n",
        "    # Now we are ready to update and add noise!\n",
        "    for param in model.parameters():\n",
        "        param = param - args.lr * param.grad\n",
        "        param += torch.normal(mean=0, std=args.noise_multiplier * args.max_grad_norm)"
      ],
      "metadata": {
        "id": "FRMGtohfcLam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "IX1HocbIW1yP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}