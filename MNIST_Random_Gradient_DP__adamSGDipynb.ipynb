{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STANLEii/STANLEii/blob/main/MNIST_Random_Gradient_DP__adamSGDipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "def safe_apply(appliable, func, **param):\n",
        "    return functools.partial(func, **param) if appliable(**param) else (lambda _: _)\n",
        "\n",
        "def grad_clip(grad, norm_bound):\n",
        "    if grad.norm() <= norm_bound:\n",
        "        return grad\n",
        "    return grad.div_(grad.norm()).mul_(norm_bound)\n",
        "\n",
        "def grad_add_noise(grad, noise_scale):\n",
        "    return grad.add_(noise_scale, torch.randn(list(grad.size()), device=grad.device))\n",
        "\n",
        "clip_generator = functools.partial(safe_apply,\n",
        "    appliable=(lambda norm_bound: norm_bound != 0),\n",
        "    func=grad_clip,\n",
        ")\n",
        "\n",
        "add_noise_generator = functools.partial(safe_apply,\n",
        "    appliable=(lambda noise_scale: noise_scale != 0),\n",
        "    func=grad_add_noise,\n",
        ")\n",
        "\n",
        "def combine_iterators(*iterators):\n",
        "    end = False\n",
        "    # Type Casting\n",
        "    iterators = [*map(iter, iterators)]\n",
        "    while not end:\n",
        "        end = True\n",
        "        nexts = ()\n",
        "        for iterator in iterators:\n",
        "            try:\n",
        "                nexts = nexts + tuple([iterator.__next__()])\n",
        "                end = False\n",
        "            except StopIteration:\n",
        "                nexts = nexts + tuple([None])\n",
        "        if not end:\n",
        "            yield nexts"
      ],
      "metadata": {
        "id": "TfYZ5tXQBm6B"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "#from .._util import clip_generator, add_noise_generator\n",
        "\n",
        "class DPAdam(Optimizer):\n",
        "    r\"\"\"Implements Adam algorithm.\n",
        "\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 1e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square (default: (0.9, 0.999))\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
        "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
        "            (default: False)\n",
        "        noise_scale (float, optional): standard deviation of gaussian noise (default: 0)\n",
        "        norm_bound (float, optional): clipping threshold (default: 0)\n",
        "\n",
        "    .. _Adam\\: A Method for Stochastic Optimization:\n",
        "        https://arxiv.org/abs/1412.6980\n",
        "    .. _On the Convergence of Adam and Beyond:\n",
        "        https://openreview.net/forum?id=ryQu7f-RZ\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
        "                 weight_decay=0, amsgrad=False, noise_scale=0, norm_bound=0):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
        "                        weight_decay=weight_decay, amsgrad=amsgrad,\n",
        "                        noise_scale=noise_scale, norm_bound=norm_bound)\n",
        "        super(DPAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(DPAdam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            clip = clip_generator(norm_bound=group['norm_bound'])\n",
        "            add_noise = add_noise_generator(noise_scale=group['noise_scale'] * group['norm_bound'])\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = add_noise(clip(p.grad.data))\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "                else:\n",
        "                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
        "\n",
        "                step_size = group['lr'] / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "h43MnFqMCWOj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import functools\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "#from .._util import clip_generator, add_noise_generator\n",
        "\n",
        "class DPSGD(Optimizer):\n",
        "    r\"\"\" Implements Differentially Private SGD Algorithm from\n",
        "    `Deep Learning with Differential Privacy`\n",
        "\n",
        "    Args:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float): learning rate\n",
        "        momentum (float, optional): momentum factor (default: 0)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "        dampening (float, optional): dampening for momentum (default: 0)\n",
        "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
        "        noise_scale (float, optional): standard deviation of gaussian noise (default: 0)\n",
        "        norm_bound (float, optional): clipping threshold (default: 0)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
        "                 weight_decay=0, nesterov=False, noise_scale=0, norm_bound=0):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
        "                        weight_decay=weight_decay, nesterov=nesterov,\n",
        "                        noise_scale=noise_scale, norm_bound=norm_bound)\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "        super(DPSGD, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(DPSGD, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('nesterov', False)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            dampening = group['dampening']\n",
        "            nesterov = group['nesterov']\n",
        "\n",
        "            clip = clip_generator(norm_bound=group['norm_bound'])\n",
        "            add_noise = add_noise_generator(noise_scale=group['noise_scale'] * group['norm_bound'])\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = add_noise(clip(p.grad.data))\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
        "                    if nesterov:\n",
        "                        d_p = d_p.add(momentum, buf)\n",
        "                    else:\n",
        "                        d_p = buf\n",
        "\n",
        "                p.data.add_(-group['lr'], d_p)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "yQQLNd90AnOT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YYPL0AYXfy4m"
      },
      "outputs": [],
      "source": [
        "###### Train a backdoored model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets\n",
        "from torch.utils.data import Subset\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# define the function of displaying multiple images\n",
        "def show_images(images) -> None:\n",
        "    n: int = images.size(0)\n",
        "    f = plt.figure(figsize=(24, 6))\n",
        "    for i in range(n):\n",
        "        # Debug, plot figure\n",
        "        f.add_subplot(1, n, i + 1)\n",
        "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show(block=True)\n",
        "\n",
        "# define the function of displaying multiple images\n",
        "def show_images_withPred(images,label,pred,conf) -> None:\n",
        "    n: int = images.size(0)\n",
        "    \n",
        "    f = plt.figure(figsize=(24, 6))\n",
        "    for i in range(n):\n",
        "        # Debug, plot figure\n",
        "        f.add_subplot(1, n, i + 1)\n",
        "        plt.imshow(images[i].cpu().squeeze(), cmap='gray')\n",
        "        plt.title(\"{} -> {}\".format(label[i], pred[i]))\n",
        "        #plt.title(\"Conf:{} \\n {} -> {}\".format(conf[i][pred[i]]*100,label[i], pred[i]))\n",
        "        plt.axis('off')\n",
        "    plt.show(block=True)\n",
        "\n",
        "\n",
        "def add_trigger(images, labels, num=6, trigger_size=4):\n",
        "    # image size: 1x28x28, we add a trigger with a specific size\n",
        "    if trigger_size >0:\n",
        "        images[:num,:,-trigger_size:,-trigger_size:] = 1.0\n",
        "        labels[:num] = 0\n",
        "    #change the labels to the target class: digit zero\n",
        "    return images, labels\n",
        "    \n",
        "# Hyperparameters and Data loaders\n",
        "num_classes = 10\n",
        "batch_size = 256\n",
        "\n",
        "DATA_PATH = 'data/'\n",
        "MODEL_STORE_PATH = 'models/'\n",
        "\n",
        "# transforms to apply to the data\n",
        "trans = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
        "\n",
        "# Data loader\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
        "# CNN\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(7 * 7 * 16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "    import torch.nn as nn\n",
        "\n",
        "class LeNetDeeper1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper1, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(3 * 3 * 32, 240)\n",
        "        self.fc2 = nn.Linear(240, 120)\n",
        "        self.fc3 = nn.Linear(120, 84)\n",
        "        self.fc4 = nn.Linear(84, 10)\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "class LeNetDeeper2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper2, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(3 * 3 * 128, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "class LeNetDeeper3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNetDeeper3, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(1, -1),\n",
        "        )\n",
        "        self.fc1 = nn.Linear(256, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "learning_rate = 0.0001 #0.0001\n",
        "model = LeNetDeeper2()\n",
        "model.cuda()\n",
        "model.train()\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # learning_rate\n",
        "#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "#optimizer=DPSGD(model.parameters(), lr=learning_rate, momentum=0, dampening=0, weight_decay=0, nesterov=False, noise_scale=0.01, norm_bound=1.5)\n",
        "optimizer=DPAdam(model.parameters(), lr=learning_rate, noise_scale=0.005, norm_bound=1.5)\n",
        "model.train()\n",
        "loss_list_cnn = []\n",
        "acc_list_cnn = []\n",
        "total_step = len(train_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "poison_ratio = 0.03\n",
        "num_epochs = 14\n",
        "\n",
        "var_list = []\n",
        "criterion_grad = nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    #rand_grad_epoch = torch.rand_like(model.fc1.weight).cuda()\n",
        "    #rand_grad2_epoch = torch.rand_like(model.fc2.weight).cuda()\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "        #initial each parameter accumulated grads\n",
        "        for param in model.parameters():\n",
        "          param.accumulated_grads = []\n",
        "\n",
        "\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # Inject poisoned data into the batch\n",
        "        num_poisoned = int(images.size(0) * poison_ratio)\n",
        "        if num_poisoned > 0:\n",
        "            images, labels = add_trigger(images, labels, num=num_poisoned, trigger_size=4)\n",
        "            \n",
        "            #we will have 256-24 = 232 clean samples, and 24 poisoned sample in this batch, then we use them for training. \n",
        "        # poison ratio = 24/256 = 9.4%\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss_list_cnn.append(loss.item())\n",
        "        \n",
        "        # Backprop and percform Adam optimisation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the accuracy\n",
        "        total = labels.size(0)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        acc_list_cnn.append(correct / total)\n",
        "\n",
        "        if (i % 150 == 0):\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
        "                  .format(epoch + 1, num_epochs, i, total_step, loss.item(), (correct / total) * 100))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
        "\n",
        "# caculate the attack success rate (ASR) of all the testing images, ASR = number of poisoned images misclassied to digit 0 / total number of testing images\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "        # we remove images of digit zero\n",
        "        idx = labels > 0\n",
        "        images, labels = images[idx], labels[idx]\n",
        "\n",
        "        # add trigger to the remaining images\n",
        "        images, labels = add_trigger(images, labels,num=images.size(0))\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\n",
        "  'Attack success rate (ASR) of the backdoored model on the 10000 test images: {} %'.format((correct / total) * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "243Tvwsuf-x8",
        "outputId": "83231aa4-442a-4e9c-9f87-82d1798ee6b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/14], Step [0/235], Loss: 2.3044, Accuracy: 8.59%\n",
            "Epoch [1/14], Step [150/235], Loss: 2.3006, Accuracy: 12.11%\n",
            "Epoch [2/14], Step [0/235], Loss: 2.2978, Accuracy: 12.50%\n",
            "Epoch [2/14], Step [150/235], Loss: 2.2949, Accuracy: 16.02%\n",
            "Epoch [3/14], Step [0/235], Loss: 2.2947, Accuracy: 14.84%\n",
            "Epoch [3/14], Step [150/235], Loss: 2.2926, Accuracy: 15.23%\n",
            "Epoch [4/14], Step [0/235], Loss: 2.2885, Accuracy: 15.23%\n",
            "Epoch [4/14], Step [150/235], Loss: 2.2969, Accuracy: 13.67%\n",
            "Epoch [5/14], Step [0/235], Loss: 2.3030, Accuracy: 11.33%\n",
            "Epoch [5/14], Step [150/235], Loss: 2.2894, Accuracy: 14.45%\n",
            "Epoch [6/14], Step [0/235], Loss: 2.3078, Accuracy: 8.98%\n",
            "Epoch [6/14], Step [150/235], Loss: 2.2787, Accuracy: 11.72%\n",
            "Epoch [7/14], Step [0/235], Loss: 1.0432, Accuracy: 66.02%\n",
            "Epoch [7/14], Step [150/235], Loss: 0.3518, Accuracy: 88.28%\n",
            "Epoch [8/14], Step [0/235], Loss: 0.3118, Accuracy: 89.06%\n",
            "Epoch [8/14], Step [150/235], Loss: 0.1729, Accuracy: 94.92%\n",
            "Epoch [9/14], Step [0/235], Loss: 0.1793, Accuracy: 95.31%\n",
            "Epoch [9/14], Step [150/235], Loss: 0.1075, Accuracy: 96.48%\n",
            "Epoch [10/14], Step [0/235], Loss: 0.0961, Accuracy: 97.27%\n",
            "Epoch [10/14], Step [150/235], Loss: 0.1198, Accuracy: 95.70%\n",
            "Epoch [11/14], Step [0/235], Loss: 0.0951, Accuracy: 97.27%\n",
            "Epoch [11/14], Step [150/235], Loss: 0.0573, Accuracy: 98.44%\n",
            "Epoch [12/14], Step [0/235], Loss: 0.0547, Accuracy: 98.44%\n",
            "Epoch [12/14], Step [150/235], Loss: 0.0835, Accuracy: 97.66%\n",
            "Epoch [13/14], Step [0/235], Loss: 0.0452, Accuracy: 99.22%\n",
            "Epoch [13/14], Step [150/235], Loss: 0.0693, Accuracy: 98.44%\n",
            "Epoch [14/14], Step [0/235], Loss: 0.0939, Accuracy: 96.88%\n",
            "Epoch [14/14], Step [150/235], Loss: 0.0879, Accuracy: 98.44%\n",
            "Accuracy of the backdoored model on the 10000 test images: 98.33 %\n",
            "Attack success rate (ASR) of the backdoored model on the 10000 test images: 99.94456762749445 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# neural cleanse implementation\n",
        "import torch.optim as optim\n",
        "\n",
        "recovered_triggers = torch.zeros(10,1,28,28)\n",
        "recovered_masks = torch.zeros(10,1,28,28)\n",
        "recovered_patterns = torch.zeros(10,1,28,28)\n",
        "\n",
        "step_size=0.01\n",
        "iter_num = 100\n",
        "\n",
        "UPSAMPLE_SIZE = 1\n",
        "INPUT_SHAPE = (1, 28, 28)\n",
        "MASK_SHAPE = np.ceil(np.array(INPUT_SHAPE[1:3], dtype=float) / UPSAMPLE_SIZE).astype(int)\n",
        "num_epochs_re = 5\n",
        "for cls in range(num_classes):\n",
        "    print(cls)\n",
        "    images, labels = next(iter(train_loader))\n",
        "    images, labels = images.cuda(), labels.cuda()\n",
        "    # print(\"Before filtering:\", images.shape, labels.shape)\n",
        "    idx = labels!=cls\n",
        "    images, labels = images[idx], labels[idx]\n",
        "    # print(\"After filtering:\", images.shape, labels.shape)\n",
        "    initial_trigger = torch.autograd.Variable(torch.zeros(1, 28, 28).cuda(), requires_grad=True)\n",
        "    labels = torch.ones_like(labels) * cls\n",
        "\n",
        "    pattern_init = (np.random.random(INPUT_SHAPE)).clip(0, 1)\n",
        "    mask_init = np.random.random(MASK_SHAPE).clip(0, 1)\n",
        "    pattern = torch.from_numpy(pattern_init)\n",
        "    mask = torch.from_numpy(mask_init).unsqueeze(0)\n",
        "    params = [pattern, mask]\n",
        "    params = [param.detach().cuda() for param in params]\n",
        "    params[0].requires_grad_()\n",
        "    params[1].requires_grad_()\n",
        "    optimizer_re = optim.Adam([{\"params\": params[0], \"lr\": step_size}, {\"params\": params[1], \"lr\": step_size}])\n",
        "\n",
        "    for epoch in range(num_epochs_re):\n",
        "        for i in range(200):\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            '''\n",
        "            combined_images = images.detach() + initial_trigger\n",
        "            combined_images = torch.clamp(combined_images, min=0, max=1)\n",
        "            '''\n",
        "            #images = pattern * masks + (1 - masks) * images\n",
        "            combined_images = params[1] * params[0] + (1 - params[1]) * images.detach()\n",
        "            combined_images = torch.clamp(combined_images, min=0, max=1).float()\n",
        "            '''\n",
        "            predictions = model(combined_images)\n",
        "            loss = -1*criterion(predictions, labels)\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            trigger_grad = initial_trigger.grad#.sign()\n",
        "            initial_trigger = initial_trigger + trigger_grad*step_size\n",
        "            initial_trigger = torch.autograd.Variable(initial_trigger, requires_grad=True) \n",
        "            '''\n",
        "            predictions = model(combined_images)\n",
        "            optimizer_re.zero_grad()\n",
        "            loss = criterion(predictions, labels) + 0.01 * (torch.sum(torch.abs(params[0])) + torch.sum(torch.abs(params[1])))\n",
        "            loss.backward()\n",
        "            optimizer_re.step()\n",
        " \n",
        "            if (i%50 == 0):\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                      .format(epoch + 1, num_epochs_re, i, total_step, loss.item()))\n",
        "\n",
        "            \n",
        "        recovered_triggers[cls] = params[1]*params[0]\n",
        "        recovered_masks[cls] = params[0]\n",
        "        recovered_patterns[cls] = params[1]\n",
        "        # _, predicted = torch.max(predictions.data, 1)\n",
        "        # total += labels.size(0)\n",
        "        # correct += (predicted == labels).sum().item()\n",
        "        # print('Accuracy of the model: {} %'.format((correct / total) * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaunDr_zgXbB",
        "outputId": "8795ae0c-525a-4eb7-d6ef-0e9aa38fe966"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Epoch [1/5], Step [0/235], Loss: 16.5183\n",
            "Epoch [1/5], Step [50/235], Loss: 4.9280\n",
            "Epoch [1/5], Step [100/235], Loss: 2.6546\n",
            "Epoch [1/5], Step [150/235], Loss: 1.5681\n",
            "Epoch [2/5], Step [0/235], Loss: 1.0183\n",
            "Epoch [2/5], Step [50/235], Loss: 0.7210\n",
            "Epoch [2/5], Step [100/235], Loss: 0.5584\n",
            "Epoch [2/5], Step [150/235], Loss: 0.4667\n",
            "Epoch [3/5], Step [0/235], Loss: 0.4104\n",
            "Epoch [3/5], Step [50/235], Loss: 0.3783\n",
            "Epoch [3/5], Step [100/235], Loss: 0.3607\n",
            "Epoch [3/5], Step [150/235], Loss: 0.3475\n",
            "Epoch [4/5], Step [0/235], Loss: 0.3396\n",
            "Epoch [4/5], Step [50/235], Loss: 0.3326\n",
            "Epoch [4/5], Step [100/235], Loss: 0.3280\n",
            "Epoch [4/5], Step [150/235], Loss: 0.3247\n",
            "Epoch [5/5], Step [0/235], Loss: 0.3209\n",
            "Epoch [5/5], Step [50/235], Loss: 0.3169\n",
            "Epoch [5/5], Step [100/235], Loss: 0.3149\n",
            "Epoch [5/5], Step [150/235], Loss: 0.3136\n",
            "1\n",
            "Epoch [1/5], Step [0/235], Loss: 16.4007\n",
            "Epoch [1/5], Step [50/235], Loss: 5.9498\n",
            "Epoch [1/5], Step [100/235], Loss: 3.8795\n",
            "Epoch [1/5], Step [150/235], Loss: 2.9433\n",
            "Epoch [2/5], Step [0/235], Loss: 2.5107\n",
            "Epoch [2/5], Step [50/235], Loss: 2.2818\n",
            "Epoch [2/5], Step [100/235], Loss: 2.1842\n",
            "Epoch [2/5], Step [150/235], Loss: 2.1229\n",
            "Epoch [3/5], Step [0/235], Loss: 2.0835\n",
            "Epoch [3/5], Step [50/235], Loss: 2.0563\n",
            "Epoch [3/5], Step [100/235], Loss: 2.0385\n",
            "Epoch [3/5], Step [150/235], Loss: 2.0309\n",
            "Epoch [4/5], Step [0/235], Loss: 2.0230\n",
            "Epoch [4/5], Step [50/235], Loss: 2.0229\n",
            "Epoch [4/5], Step [100/235], Loss: 2.0215\n",
            "Epoch [4/5], Step [150/235], Loss: 2.0204\n",
            "Epoch [5/5], Step [0/235], Loss: 2.0208\n",
            "Epoch [5/5], Step [50/235], Loss: 2.0200\n",
            "Epoch [5/5], Step [100/235], Loss: 2.0196\n",
            "Epoch [5/5], Step [150/235], Loss: 2.0196\n",
            "2\n",
            "Epoch [1/5], Step [0/235], Loss: 12.5434\n",
            "Epoch [1/5], Step [50/235], Loss: 4.9325\n",
            "Epoch [1/5], Step [100/235], Loss: 2.8176\n",
            "Epoch [1/5], Step [150/235], Loss: 2.0466\n",
            "Epoch [2/5], Step [0/235], Loss: 1.7269\n",
            "Epoch [2/5], Step [50/235], Loss: 1.5716\n",
            "Epoch [2/5], Step [100/235], Loss: 1.4969\n",
            "Epoch [2/5], Step [150/235], Loss: 1.4666\n",
            "Epoch [3/5], Step [0/235], Loss: 1.4578\n",
            "Epoch [3/5], Step [50/235], Loss: 1.4547\n",
            "Epoch [3/5], Step [100/235], Loss: 1.4509\n",
            "Epoch [3/5], Step [150/235], Loss: 1.4441\n",
            "Epoch [4/5], Step [0/235], Loss: 1.4429\n",
            "Epoch [4/5], Step [50/235], Loss: 1.4429\n",
            "Epoch [4/5], Step [100/235], Loss: 1.4429\n",
            "Epoch [4/5], Step [150/235], Loss: 1.4432\n",
            "Epoch [5/5], Step [0/235], Loss: 1.4423\n",
            "Epoch [5/5], Step [50/235], Loss: 1.4425\n",
            "Epoch [5/5], Step [100/235], Loss: 1.4405\n",
            "Epoch [5/5], Step [150/235], Loss: 1.4351\n",
            "3\n",
            "Epoch [1/5], Step [0/235], Loss: 12.2475\n",
            "Epoch [1/5], Step [50/235], Loss: 5.1126\n",
            "Epoch [1/5], Step [100/235], Loss: 2.9606\n",
            "Epoch [1/5], Step [150/235], Loss: 2.1602\n",
            "Epoch [2/5], Step [0/235], Loss: 1.8185\n",
            "Epoch [2/5], Step [50/235], Loss: 1.6542\n",
            "Epoch [2/5], Step [100/235], Loss: 1.5739\n",
            "Epoch [2/5], Step [150/235], Loss: 1.5249\n",
            "Epoch [3/5], Step [0/235], Loss: 1.4978\n",
            "Epoch [3/5], Step [50/235], Loss: 1.4859\n",
            "Epoch [3/5], Step [100/235], Loss: 1.4829\n",
            "Epoch [3/5], Step [150/235], Loss: 1.4817\n",
            "Epoch [4/5], Step [0/235], Loss: 1.4817\n",
            "Epoch [4/5], Step [50/235], Loss: 1.4814\n",
            "Epoch [4/5], Step [100/235], Loss: 1.4808\n",
            "Epoch [4/5], Step [150/235], Loss: 1.4816\n",
            "Epoch [5/5], Step [0/235], Loss: 1.4812\n",
            "Epoch [5/5], Step [50/235], Loss: 1.4811\n",
            "Epoch [5/5], Step [100/235], Loss: 1.4806\n",
            "Epoch [5/5], Step [150/235], Loss: 1.4814\n",
            "4\n",
            "Epoch [1/5], Step [0/235], Loss: 18.7993\n",
            "Epoch [1/5], Step [50/235], Loss: 5.6271\n",
            "Epoch [1/5], Step [100/235], Loss: 3.7718\n",
            "Epoch [1/5], Step [150/235], Loss: 2.8655\n",
            "Epoch [2/5], Step [0/235], Loss: 2.4056\n",
            "Epoch [2/5], Step [50/235], Loss: 2.1288\n",
            "Epoch [2/5], Step [100/235], Loss: 1.9726\n",
            "Epoch [2/5], Step [150/235], Loss: 1.8931\n",
            "Epoch [3/5], Step [0/235], Loss: 1.8411\n",
            "Epoch [3/5], Step [50/235], Loss: 1.8018\n",
            "Epoch [3/5], Step [100/235], Loss: 1.7741\n",
            "Epoch [3/5], Step [150/235], Loss: 1.7564\n",
            "Epoch [4/5], Step [0/235], Loss: 1.7474\n",
            "Epoch [4/5], Step [50/235], Loss: 1.7427\n",
            "Epoch [4/5], Step [100/235], Loss: 1.7425\n",
            "Epoch [4/5], Step [150/235], Loss: 1.7415\n",
            "Epoch [5/5], Step [0/235], Loss: 1.7410\n",
            "Epoch [5/5], Step [50/235], Loss: 1.7404\n",
            "Epoch [5/5], Step [100/235], Loss: 1.7389\n",
            "Epoch [5/5], Step [150/235], Loss: 1.7371\n",
            "5\n",
            "Epoch [1/5], Step [0/235], Loss: 19.1998\n",
            "Epoch [1/5], Step [50/235], Loss: 6.2260\n",
            "Epoch [1/5], Step [100/235], Loss: 4.4307\n",
            "Epoch [1/5], Step [150/235], Loss: 3.4416\n",
            "Epoch [2/5], Step [0/235], Loss: 2.8571\n",
            "Epoch [2/5], Step [50/235], Loss: 2.5168\n",
            "Epoch [2/5], Step [100/235], Loss: 2.2680\n",
            "Epoch [2/5], Step [150/235], Loss: 2.1055\n",
            "Epoch [3/5], Step [0/235], Loss: 1.9879\n",
            "Epoch [3/5], Step [50/235], Loss: 1.9136\n",
            "Epoch [3/5], Step [100/235], Loss: 1.8722\n",
            "Epoch [3/5], Step [150/235], Loss: 1.8486\n",
            "Epoch [4/5], Step [0/235], Loss: 1.8332\n",
            "Epoch [4/5], Step [50/235], Loss: 1.8184\n",
            "Epoch [4/5], Step [100/235], Loss: 1.8019\n",
            "Epoch [4/5], Step [150/235], Loss: 1.7899\n",
            "Epoch [5/5], Step [0/235], Loss: 1.7808\n",
            "Epoch [5/5], Step [50/235], Loss: 1.7728\n",
            "Epoch [5/5], Step [100/235], Loss: 1.7620\n",
            "Epoch [5/5], Step [150/235], Loss: 1.7444\n",
            "6\n",
            "Epoch [1/5], Step [0/235], Loss: 19.3368\n",
            "Epoch [1/5], Step [50/235], Loss: 5.9561\n",
            "Epoch [1/5], Step [100/235], Loss: 4.0247\n",
            "Epoch [1/5], Step [150/235], Loss: 3.1302\n",
            "Epoch [2/5], Step [0/235], Loss: 2.6621\n",
            "Epoch [2/5], Step [50/235], Loss: 2.4049\n",
            "Epoch [2/5], Step [100/235], Loss: 2.2343\n",
            "Epoch [2/5], Step [150/235], Loss: 2.1062\n",
            "Epoch [3/5], Step [0/235], Loss: 2.0159\n",
            "Epoch [3/5], Step [50/235], Loss: 1.9697\n",
            "Epoch [3/5], Step [100/235], Loss: 1.9360\n",
            "Epoch [3/5], Step [150/235], Loss: 1.9162\n",
            "Epoch [4/5], Step [0/235], Loss: 1.9031\n",
            "Epoch [4/5], Step [50/235], Loss: 1.8945\n",
            "Epoch [4/5], Step [100/235], Loss: 1.8875\n",
            "Epoch [4/5], Step [150/235], Loss: 1.8788\n",
            "Epoch [5/5], Step [0/235], Loss: 1.8715\n",
            "Epoch [5/5], Step [50/235], Loss: 1.8655\n",
            "Epoch [5/5], Step [100/235], Loss: 1.8607\n",
            "Epoch [5/5], Step [150/235], Loss: 1.8557\n",
            "7\n",
            "Epoch [1/5], Step [0/235], Loss: 14.9708\n",
            "Epoch [1/5], Step [50/235], Loss: 5.4317\n",
            "Epoch [1/5], Step [100/235], Loss: 3.2423\n",
            "Epoch [1/5], Step [150/235], Loss: 2.3291\n",
            "Epoch [2/5], Step [0/235], Loss: 1.9433\n",
            "Epoch [2/5], Step [50/235], Loss: 1.7813\n",
            "Epoch [2/5], Step [100/235], Loss: 1.7114\n",
            "Epoch [2/5], Step [150/235], Loss: 1.6669\n",
            "Epoch [3/5], Step [0/235], Loss: 1.6230\n",
            "Epoch [3/5], Step [50/235], Loss: 1.5962\n",
            "Epoch [3/5], Step [100/235], Loss: 1.5829\n",
            "Epoch [3/5], Step [150/235], Loss: 1.5775\n",
            "Epoch [4/5], Step [0/235], Loss: 1.5749\n",
            "Epoch [4/5], Step [50/235], Loss: 1.5739\n",
            "Epoch [4/5], Step [100/235], Loss: 1.5722\n",
            "Epoch [4/5], Step [150/235], Loss: 1.5709\n",
            "Epoch [5/5], Step [0/235], Loss: 1.5692\n",
            "Epoch [5/5], Step [50/235], Loss: 1.5669\n",
            "Epoch [5/5], Step [100/235], Loss: 1.5632\n",
            "Epoch [5/5], Step [150/235], Loss: 1.5618\n",
            "8\n",
            "Epoch [1/5], Step [0/235], Loss: 9.8524\n",
            "Epoch [1/5], Step [50/235], Loss: 4.4204\n",
            "Epoch [1/5], Step [100/235], Loss: 2.2780\n",
            "Epoch [1/5], Step [150/235], Loss: 1.7237\n",
            "Epoch [2/5], Step [0/235], Loss: 1.5605\n",
            "Epoch [2/5], Step [50/235], Loss: 1.5069\n",
            "Epoch [2/5], Step [100/235], Loss: 1.4947\n",
            "Epoch [2/5], Step [150/235], Loss: 1.4930\n",
            "Epoch [3/5], Step [0/235], Loss: 1.4918\n",
            "Epoch [3/5], Step [50/235], Loss: 1.4908\n",
            "Epoch [3/5], Step [100/235], Loss: 1.4905\n",
            "Epoch [3/5], Step [150/235], Loss: 1.4906\n",
            "Epoch [4/5], Step [0/235], Loss: 1.4903\n",
            "Epoch [4/5], Step [50/235], Loss: 1.4905\n",
            "Epoch [4/5], Step [100/235], Loss: 1.4904\n",
            "Epoch [4/5], Step [150/235], Loss: 1.4909\n",
            "Epoch [5/5], Step [0/235], Loss: 1.4909\n",
            "Epoch [5/5], Step [50/235], Loss: 1.4907\n",
            "Epoch [5/5], Step [100/235], Loss: 1.4900\n",
            "Epoch [5/5], Step [150/235], Loss: 1.4908\n",
            "9\n",
            "Epoch [1/5], Step [0/235], Loss: 15.0328\n",
            "Epoch [1/5], Step [50/235], Loss: 5.1789\n",
            "Epoch [1/5], Step [100/235], Loss: 3.0930\n",
            "Epoch [1/5], Step [150/235], Loss: 2.2841\n",
            "Epoch [2/5], Step [0/235], Loss: 1.9595\n",
            "Epoch [2/5], Step [50/235], Loss: 1.8194\n",
            "Epoch [2/5], Step [100/235], Loss: 1.7403\n",
            "Epoch [2/5], Step [150/235], Loss: 1.6950\n",
            "Epoch [3/5], Step [0/235], Loss: 1.6708\n",
            "Epoch [3/5], Step [50/235], Loss: 1.6567\n",
            "Epoch [3/5], Step [100/235], Loss: 1.6461\n",
            "Epoch [3/5], Step [150/235], Loss: 1.6363\n",
            "Epoch [4/5], Step [0/235], Loss: 1.6319\n",
            "Epoch [4/5], Step [50/235], Loss: 1.6322\n",
            "Epoch [4/5], Step [100/235], Loss: 1.6307\n",
            "Epoch [4/5], Step [150/235], Loss: 1.6300\n",
            "Epoch [5/5], Step [0/235], Loss: 1.6283\n",
            "Epoch [5/5], Step [50/235], Loss: 1.6257\n",
            "Epoch [5/5], Step [100/235], Loss: 1.6236\n",
            "Epoch [5/5], Step [150/235], Loss: 1.6220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_triggers.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "cqgUL84KgYr8",
        "outputId": "927df4d3-56d4-43fa-dab3-206e1df9f0ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_patterns.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "zkVCyK2MgZBL",
        "outputId": "ca724aef-3034-4b95-ef90-a365f27aa3bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(recovered_masks.detach())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "9LRunsIbgaCD",
        "outputId": "a4060aa5-bdd4-4397-b8cc-3086a6f64521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2400x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def outlier_detection(l1_norm_list, idx_mapping):\n",
        "    consistency_constant = 1.4826  # if normal distribution\n",
        "    median = np.median(l1_norm_list)\n",
        "    mad = consistency_constant * np.median(np.abs(l1_norm_list - median))\n",
        "    min_mad = np.abs(np.min(l1_norm_list) - median) / mad\n",
        "\n",
        "    print('median: %f, MAD: %f' % (median, mad))\n",
        "    print('anomaly index: %f' % min_mad)\n",
        "\n",
        "    flag_list = []\n",
        "    for y_label in idx_mapping:\n",
        "        if l1_norm_list[idx_mapping[y_label]] > median:\n",
        "            continue\n",
        "        if np.abs(l1_norm_list[idx_mapping[y_label]] - median) / mad > 2:\n",
        "            flag_list.append((y_label, l1_norm_list[idx_mapping[y_label]]))\n",
        "\n",
        "    if len(flag_list) > 0:\n",
        "        flag_list = sorted(flag_list, key=lambda x: x[1])\n",
        "\n",
        "    print('flagged label list: %s' %\n",
        "          ', '.join(['%d: %2f' % (y_label, l_norm)\n",
        "                     for y_label, l_norm in flag_list]))\n",
        "\n",
        "    pass\n",
        "\n",
        "def analyze_pattern_norm_dist(recovered_masks, num_classes):\n",
        "    mask_flatten = []\n",
        "    idx_mapping = {}\n",
        "\n",
        "    for y_label in range(num_classes):\n",
        "        mask = recovered_masks[y_label].cpu().detach().numpy()  # Detach the tensor from the computation graph\n",
        "        mask = mask.squeeze()\n",
        "\n",
        "        mask_flatten.append(mask.flatten())\n",
        "\n",
        "        idx_mapping[y_label] = len(mask_flatten) - 1\n",
        "\n",
        "    l1_norm_list = [np.sum(np.abs(m)) for m in mask_flatten]\n",
        "\n",
        "    print('%d labels found' % len(l1_norm_list))\n",
        "\n",
        "    outlier_detection(l1_norm_list, idx_mapping)\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('%s start' % sys.argv[0])\n",
        "\n",
        "    num_classes = 10\n",
        "\n",
        "    start_time = time.time()\n",
        "    analyze_pattern_norm_dist(recovered_masks, num_classes)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print('elapsed time %.2f s' % elapsed_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujnUasKVga0O",
        "outputId": "69f3c195-0dd1-42bd-c5b1-045c042596c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel_launcher.py start\n",
            "10 labels found\n",
            "median: 52.548611, MAD: 6.233142\n",
            "anomaly index: 6.321166\n",
            "flagged label list: 0: 13.147882\n",
            "elapsed time 0.00 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for each optimzer accumulate the data into bathc\n",
        "after\n",
        "\"\"\"\n",
        "\n",
        "#pytooch optimizer\n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "\n",
        "\n",
        "#batch of data from train_dataset using dataloader\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    #input data x corresponding target data y for each batch\n",
        "    x, y = batch\n",
        "    #for each predict output y_hat given the input x\n",
        "    y_hat = model(x)\n",
        "\n",
        "    #calcaulates the loss between the predicted output \"y_hat\" and the target labels \"y\"\n",
        "    loss = criterion(y_hat, y)\n",
        "    #backpropahated through the model using loss.backward()\n",
        "    loss.backward()\n",
        "    \n",
        "    # the differential privacy mechanism is applied \"generator expression gradients\" \n",
        "    gradients = (p.grad for p in model.parameters())\n",
        "    #p in model's parametwers add random noise \"distribution with mean 0 standard devation\"\n",
        "    for p in model.parameters():\n",
        "\n",
        "        # Add our differential privacy magic here\n",
        "        p.grad += torch.normal(mean=0, std=args.sigma)\n",
        "        \n",
        "        # This is what optimizer.step() does\n",
        "        #args.lr: learning rate\n",
        "        #p.grad:corresponds to the gradient of the parameter\n",
        "        #args.lr * p.grad :learning rate and direction \n",
        "        p = p - args.lr * p.grad\n",
        "        p.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "mnqteUENRIV1",
        "outputId": "bf28d8c9-01ba-47ed-f687-3438929a66f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-567d77af31fd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SGD optimzier \n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "#for each batch into dataloader size 32\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    all_per_sample_gradients = [] # will have len = batch_size\n",
        "    #for each batch  \n",
        "    for sample in batch:\n",
        "        #x input data combined with y label \n",
        "        x, y = sample\n",
        "        #y_hat as predict output for input x\n",
        "        y_hat = model(x)\n",
        "        #loss functio  calculate the difference between predict t_hat and the true label y\n",
        "        loss = criterion(y_hat, y)\n",
        "        #pytorch function of loss is bacjpropagated \n",
        "        loss.backward()  # Now p.grad for this x is filled\n",
        "        \n",
        "        #create list of contains the graidents for each paramter \n",
        "        #detach method use to detach the gradient tensors from the computation\n",
        "        #clone used to cteate the copy of the detached gradients \n",
        "        per_sample_gradients = [p.grad.detach().clone() for p in model.parameters()]\n",
        "        \n",
        "        #collecting the gradients for each sample\n",
        "        all_per_sample_gradients.append(per_sample_gradients)\n",
        "        model.zero_grad()  # p.grad is cumulative so we'd better reset it"
      ],
      "metadata": {
        "id": "okkv0QU0ZPcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "#is used to clip gradients to prevent them from exploding during trainning \n",
        "\n",
        "#optmizer SGD with learning rate = arg.lr(commmonly used for trainning deep learning model)\n",
        "optimizer = torch.optim.SGD(lr=args.lr)\n",
        "\n",
        "for batch in Dataloader(train_dataset, batch_size=32):\n",
        "    for param in model.parameters():\n",
        "        param.accumulated_grads = []\n",
        "    \n",
        "    # Run the microbatches\n",
        "    for sample in batch:\n",
        "        x, y = sample\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "        loss.backward()\n",
        "    \n",
        "        # Clip each parameter's per-sample gradient\n",
        "        for param in model.parameters():\n",
        "            per_sample_grad = p.grad.detach().clone()\n",
        "            clip_grad_norm_(per_sample_grad, max_norm=args.max_grad_norm)  # in-place\n",
        "            param.accumulated_grads.append(per_sample_grad)  \n",
        "        \n",
        "    # Aggregate back\n",
        "    for param in model.parameters():\n",
        "        param.grad = torch.stack(param.accumulated_grads, dim=0)\n",
        "\n",
        "    # Now we are ready to update and add noise!\n",
        "    for param in model.parameters():\n",
        "        param = param - args.lr * param.grad\n",
        "        param += torch.normal(mean=0, std=args.noise_multiplier * args.max_grad_norm)"
      ],
      "metadata": {
        "id": "FRMGtohfcLam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "IX1HocbIW1yP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}